# -*- coding: utf-8 -*-
"""tweet-sentiment-naive-bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQ4EOMPAA2DV5T3nq6nOHJP3k9Kqr_Ps
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score

"""# Building Naive Bayes (Multinomial) from scratch"""

class NaiveBayes(): #multinomial naive bayes
  def __init__(self,alpha=1.0):
    self.alpha=alpha
  def fit(self,X,Y):
    self.n_classes=len(np.unique(Y))
    self.n_features=X.shape[1] # V (unique vocab)
    self.p_classes=np.zeros((self.n_classes,1))
    self.m=X.shape[0] # no of docs / training examples
    self.feature_count=np.zeros((self.n_classes,self.n_features))
    for i in range(self.n_classes):
      x_class_i=X[Y==i]
      self.feature_count[i]=np.sum(x_class_i, axis=0)
      self.p_classes[i]=x_class_i.shape[0]/self.m
    self.log_probs=np.zeros((self.n_classes,self.n_features))
    self.log_probs=np.log((self.feature_count+self.alpha)/((np.sum(self.feature_count,axis=1,keepdims=True))+(self.alpha*self.n_features))) # log((count of the word in that class + alpha)/ total number of words in that class + alpha*|V|)
    self.log_class=np.log(self.p_classes).flatten()
  def predict(self,X):
    y_pred=np.argmax(self.predict_probabilities(X),axis=1)
    return y_pred
  def predict_probabilities(self,X):
    # shape of X is (n_examples,n_features) , shape of log_probs.T is (n_features, n_classes) , so their dot product will have the shape (n_examples,n_features), then we pick the max probability
    return np.dot(X,self.log_probs.T)+ self.log_class # log probability of class c + sum of log probabilities of each word in the document given this class c

  def score(self,X,Y):
    y_pred=self.predict(X)
    return accuracy_score(Y,y_pred)

"""# NLP Preprocessing for tweets to test the custom model

### Data Overview
"""

tweets=pd.read_csv('/content/train.csv',encoding='utf-8')

tweets.info()

tweets.drop(columns=['textID','selected_text'],inplace=True)

tweets['sentiment']=tweets['sentiment'].map({'negative':0,'positive':1})

tweets['sentiment'].value_counts(normalize=True)

tweets=tweets.loc[tweets['sentiment']!='neutral']

tweets.shape

tweets.isnull().sum()

tweets.loc[tweets["text"].isnull(),:]

tweets.dropna(axis=0, inplace=True)

tweets.sample(3)

df=tweets.copy(deep=True)

df['character_length']=df['text'].apply(len)
df['word_count']=df['text'].apply(lambda x: len(x.split()))

print(df[['character_length','word_count']].describe())

import re

print("Number of tweets containing urls:", tweets['text'].str.contains(r"https?://\w+").sum())
print("Number of tweets containing mentions:", tweets['text'].str.contains(r"@\w+").sum())
print("Number of tweets containing hashtags: ", tweets['text'].str.contains(r"#\w+").sum())
print("Number of tweets containing html tags (bc data is scraped): ", tweets['text'].str.contains(r"<[^>]>").sum())
print("Number of tweets containing emojis(using emoji unicode)",tweets['text'].str.contains('[^\x00-\x7F]').sum())

!pip install ftfy

import ftfy
tweets['text'] = tweets['text'].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)
tweets['text'] = tweets['text'].str.replace('`', "'", regex=False)

mask = tweets['text'].str.contains('\ufffd', na=False)
tweets = tweets[~mask].reset_index(drop=True)

"""### Data Cleaning"""

## removing html using bs4
from bs4 import BeautifulSoup
def remove_html(text):
  return BeautifulSoup(text).get_text()

tweets['text']=tweets['text'].apply(remove_html)

## removing urls:
def remove_urls(text):
  text=re.sub(r"https?://\S+",'',text)
  text=re.sub(r"www\.\S+",'',text)
  return text

tweets['text']=tweets['text'].apply(remove_urls)

## remove mentions:
def remove_mentions(text):
  text=re.sub(r"@\w+",'',text)
  return text

tweets['text']=tweets['text'].apply(remove_mentions)

## remove hashtags:
# will keep the word itself for context
def remove_hashtags(text):
  text=re.sub(r"#(\w+)",r"\1",text)
  return text

tweets['text']=tweets['text'].apply(remove_hashtags)

# handle emojis (will turn into words instead of removing them totally)
!pip install emoji

import emoji

def extract_emojis(text):
    return emoji.demojize(text, delimiters=(' ', ' ')).replace('_',' ').replace('-',' ')

tweets['text']=tweets['text'].apply(extract_emojis)

!pip install contractions

## handling contractions (can't/don't/won't) before removing punctuation
import contractions

tweets['text']=tweets['text'].apply(lambda x: contractions.fix(x))

import string

def remove_punctuation(text):
  text = text.translate(str.maketrans('', '', string.punctuation))
  return text

tweets['text']=tweets['text'].apply(remove_punctuation)

tweets['text']=tweets['text'].apply(lambda x: x.lower())

## removing numbers: (wont matter in sentimenr):
def remove_numbers(text):
  return re.sub(r"\b\d+\b",'',text)

tweets['text']=tweets['text'].apply(remove_numbers)

def remove_repeated_chars(text):  # remove repeated chars zay yaaaayyy and so on
  text=re.sub(r"(.)\1{2,}",r"\1\1",text)
  return text

tweets['text']=tweets['text'].apply(remove_repeated_chars)

# normalize extra whitespaces:
def remove_extra_whitespaces(text):
  text=re.sub(r"\s+",' ',text).strip()
  return text

tweets['text']=tweets['text'].apply(remove_extra_whitespaces)

print(tweets.sample(5))

"""### Tokenization"""

import nltk
nltk.download('punkt',quiet=True)
nltk.download('punkt_tab',quiet=True)
nltk.download('stopwords',quiet=True)
nltk.download('wordnet',quiet=True)
nltk.download('averaged_perceptron_tagger_eng',quiet=True)

!pip install nltk spacy

from nltk.tokenize import word_tokenize

tweets['text']=tweets['text'].apply(word_tokenize)

"""### Stopword Removal"""

from nltk.corpus import stopwords

# wont be removing negation bc it's important
stop_words=set(stopwords.words('english'))

negative_words={'no', 'not', 'nor', 'neither', 'never', 'nobody', 'nothing','nowhere', 'cannot','can\'t','won\'t'}

neg_in_stopwords=stop_words.intersection(negative_words)

stop_words_without_negation=stop_words-neg_in_stopwords

neg_in_stopwords

def remove_stop_words(text):
  return [token for token in text if token not in stop_words_without_negation]

tweets['text']=tweets['text'].apply(remove_stop_words)

"""### POS and Lemmatization"""

tweets.sample()

from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

def get_pos(word):
  if word.startswith('J'):
    return wordnet.ADJ
  elif word.startswith('V'):
    return wordnet.VERB
  elif word.startswith('N'):
    return wordnet.NOUN
  elif word.startswith('R'):
    return wordnet.ADV
  else:
    return wordnet.NOUN

wordnetlemmatizer=WordNetLemmatizer()

nltk.download('averaged_perceptron_tagger_eng',quiet=True)

def lemmatize_tokens(tokens):
    pos_tags = nltk.pos_tag(tokens)
    lemmatizations=[wordnetlemmatizer.lemmatize(word, get_pos(tag))for word, tag in pos_tags]
    return lemmatizations

tweets['text']=tweets['text'].apply(lemmatize_tokens)

tweets.sample()

tweets['text']=tweets['text'].apply(lambda x: ' '.join(x))

"""### Vectorization"""

X_train, X_test, y_train, y_test = train_test_split(tweets['text'], tweets['sentiment'], test_size=0.2,stratify=tweets['sentiment'])

count_vectorizer = CountVectorizer(ngram_range=(1, 2),max_features=15000,min_df=2,max_df=0.95)
X_train_cv = count_vectorizer.fit_transform(X_train)
X_test_cv  = count_vectorizer.transform(X_test)

X_train_cv

"""### Comparing custom Naive Bayes class against Sklearn's MultinomialNB"""

from sklearn.metrics import accuracy_score,classification_report

#finding the best alpha (for Laplacian Smoothing) using gridsearchcv
from sklearn.model_selection import GridSearchCV
params = {'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}
grid = GridSearchCV(MultinomialNB(), params, cv=5, scoring='accuracy')
grid.fit(X_train_cv, y_train)

print("Best alpha:", grid.best_params_)
print("Best CV score:", grid.best_score_)

#testing the custom NB
custom_naive_bayes=NaiveBayes(alpha=2.0)
custom_naive_bayes.fit(X_train_cv.toarray(),y_train)
y_pred=custom_naive_bayes.predict(X_test_cv.toarray())

print(classification_report(y_test, custom_naive_bayes.predict(X_test_cv.toarray())))

#testing sklearn's MultinomialNB
sklearn_nb=MultinomialNB(alpha=2.0)
sklearn_nb.fit(X_train_cv.toarray(),y_train)
y_pred=sklearn_nb.predict(X_test_cv.toarray())

print(classification_report(y_test, sklearn_nb.predict(X_test_cv.toarray())))

# Almost identical precision and recall scores!

# trying tfidf instead of BoW (count vectorizer)
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=15000, min_df=2, max_df=0.95, sublinear_tf=True)
X_train_tf = tfidf.fit_transform(X_train)
X_test_tf  = tfidf.transform(X_test)

nb_tfidf = MultinomialNB(alpha=2.0)
nb_tfidf.fit(X_train_tf, y_train)
print(classification_report(y_test, nb_tfidf.predict(X_test_tf)))

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

for ax, (preds, title) in zip(axes, [
    (custom_naive_bayes.predict(X_test_cv.toarray()),'Custom NaiveBayes'),
    (sklearn_nb.predict(X_test_cv),'sklearn MultinomialNB'),
]):
    cm = confusion_matrix(y_test, preds)
    disp = ConfusionMatrixDisplay(cm, display_labels=['negative','positive'])
    disp.plot(ax=ax, colorbar=False, cmap='Reds')
    ax.set_title(title)

plt.tight_layout()
plt.show()

feature_names = count_vectorizer.get_feature_names_out()
log_prob_diff = custom_naive_bayes.log_probs[1]-custom_naive_bayes.log_probs[0]

top_positive_idx = np.argsort(log_prob_diff)[-20:][::-1]
top_negative_idx = np.argsort(log_prob_diff)[:20]
pos=[]
print('Top 20 n-grams most associated with positive sentiment:')
for idx in top_positive_idx:
        pos.append(feature_names[idx])
print(pos)
negs=[]
print('\nTop 20 n-grams most associated with negative sentiment:')
for idx in top_negative_idx:
    negs.append(feature_names[idx])
print(negs)